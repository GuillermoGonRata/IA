{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82ec77e2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82ec77e2",
        "outputId": "32e67d70-3d03-4a9e-effa-cee19f4ed939"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU disponible: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(\"GPU disponible:\", tf.config.list_physical_devices('GPU'))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Tamaño y batch\n",
        "img_size = (48, 48)\n",
        "batch_size = 32\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "# Ruta del dataset\n",
        "dataDir = \"dataset/train\"\n",
        "\n",
        "# Carga de datos\n",
        "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    dataDir,\n",
        "    validation_split=0.2,\n",
        "    subset=\"validation\",\n",
        "    seed=123,\n",
        "    image_size=img_size,\n",
        "    color_mode=\"grayscale\",\n",
        "    batch_size=batch_size\n",
        ")\n",
        "\n",
        "original_train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    dataDir,\n",
        "    validation_split=0.2,\n",
        "    subset=\"training\",\n",
        "    seed=123,\n",
        "    image_size=img_size,\n",
        "    color_mode=\"grayscale\",\n",
        "    batch_size=batch_size\n",
        ")\n",
        "\n",
        "# Clases\n",
        "class_names = original_train_ds.class_names\n",
        "\n",
        "# Normalización\n",
        "normalization_layer = tf.keras.layers.Rescaling(1./255)\n",
        "\n",
        "# Data augmentation\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "    layers.RandomFlip(\"horizontal\"),\n",
        "    layers.RandomRotation(0.2),\n",
        "    layers.RandomZoom(0.2),\n",
        "    layers.RandomContrast(0.2),\n",
        "])\n",
        "\n",
        "# Pipeline optimizado\n",
        "train_ds = original_train_ds.cache().shuffle(1000).map(\n",
        "    lambda x, y: (data_augmentation(normalization_layer(x), training=True), y),\n",
        "    num_parallel_calls=AUTOTUNE\n",
        ").prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "val_ds = val_ds.cache().map(\n",
        "    lambda x, y: (normalization_layer(x), y),\n",
        "    num_parallel_calls=AUTOTUNE\n",
        ").prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "# Verifica GPU\n",
        "print(\"Dispositivos GPU disponibles:\", tf.config.list_physical_devices('GPU'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3X_6Eheyd30",
        "outputId": "81cf6a22-5209-4613-af08-e852f5530613"
      },
      "id": "b3X_6Eheyd30",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 28709 files belonging to 6 classes.\n",
            "Using 5741 files for validation.\n",
            "Found 28709 files belonging to 6 classes.\n",
            "Using 22968 files for training.\n",
            "Dispositivos GPU disponibles: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db70f5e0",
      "metadata": {
        "id": "db70f5e0"
      },
      "source": [
        "## Entrenamiento:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "b61eab4e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b61eab4e",
        "outputId": "1a3e1d4c-1180-437b-b92d-51df23eee707"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.1854 - loss: 3.5659\n",
            "Epoch 1: val_loss improved from inf to 1.88068, saving model to best_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 45ms/step - accuracy: 0.1854 - loss: 3.5648 - val_accuracy: 0.2007 - val_loss: 1.8807\n",
            "Epoch 2/30\n",
            "\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.1875 - loss: 1.8502\n",
            "Epoch 2: val_loss improved from 1.88068 to 1.77453, saving model to best_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 38ms/step - accuracy: 0.1875 - loss: 1.8501 - val_accuracy: 0.2054 - val_loss: 1.7745\n",
            "Epoch 3/30\n",
            "\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.1962 - loss: 1.7844\n",
            "Epoch 3: val_loss did not improve from 1.77453\n",
            "\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 37ms/step - accuracy: 0.1962 - loss: 1.7844 - val_accuracy: 0.1949 - val_loss: 1.7958\n",
            "Epoch 4/30\n",
            "\u001b[1m717/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.1892 - loss: 1.7732\n",
            "Epoch 4: val_loss did not improve from 1.77453\n",
            "\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 37ms/step - accuracy: 0.1892 - loss: 1.7732 - val_accuracy: 0.1702 - val_loss: 1.8357\n",
            "Epoch 5/30\n",
            "\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.1945 - loss: 1.7763\n",
            "Epoch 5: val_loss did not improve from 1.77453\n",
            "\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 37ms/step - accuracy: 0.1945 - loss: 1.7763 - val_accuracy: 0.1825 - val_loss: 1.8078\n",
            "Epoch 6/30\n",
            "\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.2114 - loss: 1.7640\n",
            "Epoch 6: val_loss did not improve from 1.77453\n",
            "\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 37ms/step - accuracy: 0.2114 - loss: 1.7640 - val_accuracy: 0.1843 - val_loss: 1.7933\n",
            "Epoch 7/30\n",
            "\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.2254 - loss: 1.7471\n",
            "Epoch 7: val_loss did not improve from 1.77453\n",
            "\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 37ms/step - accuracy: 0.2254 - loss: 1.7471 - val_accuracy: 0.2318 - val_loss: 1.8503\n",
            "Epoch 8/30\n",
            "\u001b[1m717/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.2556 - loss: 1.7166\n",
            "Epoch 8: val_loss improved from 1.77453 to 1.72870, saving model to best_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 38ms/step - accuracy: 0.2556 - loss: 1.7165 - val_accuracy: 0.2564 - val_loss: 1.7287\n",
            "Epoch 9/30\n",
            "\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.2769 - loss: 1.6935\n",
            "Epoch 9: val_loss improved from 1.72870 to 1.63064, saving model to best_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 38ms/step - accuracy: 0.2769 - loss: 1.6935 - val_accuracy: 0.3264 - val_loss: 1.6306\n",
            "Epoch 10/30\n",
            "\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.2956 - loss: 1.6771\n",
            "Epoch 10: val_loss did not improve from 1.63064\n",
            "\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 37ms/step - accuracy: 0.2956 - loss: 1.6771 - val_accuracy: 0.3268 - val_loss: 1.6487\n",
            "Epoch 11/30\n",
            "\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.3095 - loss: 1.6607\n",
            "Epoch 11: val_loss did not improve from 1.63064\n",
            "\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 37ms/step - accuracy: 0.3095 - loss: 1.6606 - val_accuracy: 0.3231 - val_loss: 1.7477\n",
            "Epoch 12/30\n",
            "\u001b[1m717/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.3290 - loss: 1.6353\n",
            "Epoch 12: val_loss did not improve from 1.63064\n",
            "\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 37ms/step - accuracy: 0.3290 - loss: 1.6353 - val_accuracy: 0.3268 - val_loss: 1.7753\n",
            "Epoch 13/30\n",
            "\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.3408 - loss: 1.6248\n",
            "Epoch 13: val_loss did not improve from 1.63064\n",
            "\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 37ms/step - accuracy: 0.3408 - loss: 1.6248 - val_accuracy: 0.3423 - val_loss: 1.6586\n",
            "Epoch 14/30\n",
            "\u001b[1m717/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.3531 - loss: 1.5967\n",
            "Epoch 14: val_loss improved from 1.63064 to 1.54764, saving model to best_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 37ms/step - accuracy: 0.3531 - loss: 1.5967 - val_accuracy: 0.3728 - val_loss: 1.5476\n",
            "Epoch 15/30\n",
            "\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.3662 - loss: 1.5913\n",
            "Epoch 15: val_loss improved from 1.54764 to 1.48807, saving model to best_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 38ms/step - accuracy: 0.3662 - loss: 1.5913 - val_accuracy: 0.4067 - val_loss: 1.4881\n",
            "Epoch 16/30\n",
            "\u001b[1m717/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.3689 - loss: 1.5741\n",
            "Epoch 16: val_loss did not improve from 1.48807\n",
            "\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 37ms/step - accuracy: 0.3689 - loss: 1.5741 - val_accuracy: 0.4041 - val_loss: 1.5110\n",
            "Epoch 17/30\n",
            "\u001b[1m717/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.3918 - loss: 1.5585\n",
            "Epoch 17: val_loss did not improve from 1.48807\n",
            "\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 37ms/step - accuracy: 0.3919 - loss: 1.5585 - val_accuracy: 0.3705 - val_loss: 1.5866\n",
            "Epoch 18/30\n",
            "\u001b[1m717/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.4018 - loss: 1.5347\n",
            "Epoch 18: val_loss did not improve from 1.48807\n",
            "\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 37ms/step - accuracy: 0.4017 - loss: 1.5347 - val_accuracy: 0.3520 - val_loss: 1.7387\n",
            "Epoch 19/30\n",
            "\u001b[1m717/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.4047 - loss: 1.5295\n",
            "Epoch 19: val_loss improved from 1.48807 to 1.47561, saving model to best_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 37ms/step - accuracy: 0.4047 - loss: 1.5295 - val_accuracy: 0.4241 - val_loss: 1.4756\n",
            "Epoch 20/30\n",
            "\u001b[1m717/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.4105 - loss: 1.5139\n",
            "Epoch 20: val_loss improved from 1.47561 to 1.43271, saving model to best_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 38ms/step - accuracy: 0.4105 - loss: 1.5139 - val_accuracy: 0.4440 - val_loss: 1.4327\n",
            "Epoch 21/30\n",
            "\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.4198 - loss: 1.5106\n",
            "Epoch 21: val_loss did not improve from 1.43271\n",
            "\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 39ms/step - accuracy: 0.4198 - loss: 1.5106 - val_accuracy: 0.4149 - val_loss: 1.4944\n",
            "Epoch 22/30\n",
            "\u001b[1m717/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.4228 - loss: 1.5041\n",
            "Epoch 22: val_loss did not improve from 1.43271\n",
            "\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 37ms/step - accuracy: 0.4228 - loss: 1.5041 - val_accuracy: 0.3383 - val_loss: 1.7308\n",
            "Epoch 23/30\n",
            "\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.4258 - loss: 1.4892\n",
            "Epoch 23: val_loss did not improve from 1.43271\n",
            "\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 37ms/step - accuracy: 0.4258 - loss: 1.4892 - val_accuracy: 0.3701 - val_loss: 1.6144\n",
            "Epoch 24/30\n",
            "\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.4292 - loss: 1.4842\n",
            "Epoch 24: val_loss did not improve from 1.43271\n",
            "\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 37ms/step - accuracy: 0.4292 - loss: 1.4842 - val_accuracy: 0.3802 - val_loss: 1.6016\n",
            "Epoch 25/30\n",
            "\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.4341 - loss: 1.4723\n",
            "Epoch 25: val_loss did not improve from 1.43271\n",
            "\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 39ms/step - accuracy: 0.4341 - loss: 1.4723 - val_accuracy: 0.4386 - val_loss: 1.4952\n",
            "Epoch 26/30\n",
            "\u001b[1m717/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.4336 - loss: 1.4828\n",
            "Epoch 26: val_loss did not improve from 1.43271\n",
            "\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 37ms/step - accuracy: 0.4336 - loss: 1.4828 - val_accuracy: 0.4438 - val_loss: 1.4335\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras import layers, models, regularizers\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import numpy as np\n",
        "\n",
        "train_ds = train_ds.map(lambda x, y: (data_augmentation(x,training=True), y))\n",
        "\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "    layers.RandomFlip(\"horizontal\"),\n",
        "    layers.RandomRotation(0.2),\n",
        "    layers.RandomZoom(0.2),\n",
        "    layers.RandomContrast(0.2),\n",
        "])\n",
        "\n",
        "model = models.Sequential([\n",
        "    keras.layers.Input(shape=(48, 48, 1)),\n",
        "\n",
        "    keras.layers.Conv2D(64, (3,3), activation='relu', padding='same'),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Conv2D(64, (3,3), activation='relu', padding='same'),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.MaxPooling2D((2,2)),\n",
        "\n",
        "    keras.layers.Conv2D(128, (3,3), activation='relu', padding='same'),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Conv2D(128, (3,3), activation='relu', padding='same'),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.MaxPooling2D((2,2)),\n",
        "\n",
        "    keras.layers.Conv2D(256, (3,3), activation='relu', padding='same'),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Conv2D(256, (3,3), activation='relu', padding='same'),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.MaxPooling2D((2,2)),\n",
        "\n",
        "    keras.layers.GlobalAveragePooling2D(),\n",
        "    keras.layers.Dropout(0.5),\n",
        "    keras.layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
        "    keras.layers.Dropout(0.5),\n",
        "    keras.layers.Dense(len(class_names), activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=6, restore_best_weights=True)\n",
        "checkpoint = ModelCheckpoint(\"best_model.h5\", save_best_only=True)\n",
        "\n",
        "\n",
        "y_train = []\n",
        "for _, labels in train_ds.unbatch():\n",
        "    y_train.append(labels.numpy())\n",
        "y_train = np.array(y_train)\n",
        "\n",
        "class_weights = compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.unique(y_train),\n",
        "    y=y_train\n",
        ")\n",
        "class_weights_dict = dict(enumerate(class_weights))\n",
        "\n",
        "# Callback para detener entrenamiento si no mejora\n",
        "early_stop = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=6,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "# Callback para guardar el mejor modelo\n",
        "checkpoint = ModelCheckpoint(\n",
        "    \"best_model.h5\",          # nombre del archivo\n",
        "    monitor='val_loss',       # métrica a vigilar\n",
        "    save_best_only=True,      # solo guarda si mejora\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Entrenamiento con callbacks\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=30,\n",
        "    callbacks=[early_stop, checkpoint],  # aquí incluyes ambos\n",
        "    class_weight=class_weights_dict\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Descargar el modelo:"
      ],
      "metadata": {
        "id": "WJ2rb0AG3i_r"
      },
      "id": "WJ2rb0AG3i_r"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"best_model.h5\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "5Ks08q2c3iDz",
        "outputId": "2526b2d7-95d8-4ba0-aa6c-182145a815b6"
      },
      "id": "5Ks08q2c3iDz",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_29b15ce1-6bc9-447e-9d12-d6d1dcd0fd60\", \"best_model.h5\", 14689736)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8efeb6fd",
      "metadata": {
        "id": "8efeb6fd"
      },
      "source": [
        "## Matriz de confusion:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5f0c73f",
      "metadata": {
        "id": "d5f0c73f"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import numpy as np\n",
        "\n",
        "# Predicciones\n",
        "y_pred = np.argmax(model.predict(val_ds), axis=1)\n",
        "\n",
        "# Etiquetas verdaderas\n",
        "y_true = []\n",
        "for _, labels in val_ds.unbatch():\n",
        "    y_true.append(labels.numpy())\n",
        "y_true = np.array(y_true)\n",
        "\n",
        "# Reporte\n",
        "print(confusion_matrix(y_true, y_pred))\n",
        "print(classification_report(y_true, y_pred, target_names=class_names, zero_division=0))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "79b0d897",
      "metadata": {
        "id": "79b0d897",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "36167a83-8391-4c62-dd7d-6ed27408940c"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'y_true' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4275735265.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mcm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheatmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfmt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'd'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Blues'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxticklabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myticklabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'y_true' is not defined"
          ]
        }
      ],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Matriz de Confusión')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9822b60a",
      "metadata": {
        "id": "9822b60a"
      },
      "source": [
        "## Evaluacion del accuaracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "1c8ca952",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1c8ca952",
        "outputId": "42dbd50b-fe9d-47b4-a16b-9e1bbb4616ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 7178 files belonging to 6 classes.\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.4379 - loss: 1.4423\n",
            "Test accuracy: 0.44\n"
          ]
        }
      ],
      "source": [
        "test_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    \"dataset/test\",\n",
        "    image_size=img_size,\n",
        "    color_mode=\"grayscale\",\n",
        "    batch_size=batch_size\n",
        ")\n",
        "\n",
        "test_ds = test_ds.map(lambda x, y: (normalization_layer(x), y)).cache().prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "\n",
        "test_loss, test_acc = model.evaluate(test_ds)\n",
        "print(f\"Test accuracy: {test_acc:.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6fff4e4f",
      "metadata": {
        "id": "6fff4e4f"
      },
      "source": [
        "## Grafica para ver la accuaracy del modelo:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d6f4163",
      "metadata": {
        "id": "6d6f4163"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs_range = range(len(acc))\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Train Accuracy')\n",
        "plt.plot(epochs_range, val_acc, label='Val Accuracy')\n",
        "plt.legend()\n",
        "plt.title('Accuracy')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Train Loss')\n",
        "plt.plot(epochs_range, val_loss, label='Val Loss')\n",
        "plt.legend()\n",
        "plt.title('Loss')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0569ec06",
      "metadata": {
        "id": "0569ec06"
      },
      "source": [
        "## Grafico para ver el desbalance:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b18c05c4",
      "metadata": {
        "id": "b18c05c4"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class_counts = {'angry': 3995, 'disgust': 436, 'fear': 4097, 'happy': 7215, 'neutral': 4965, 'sad': 4830, 'surprise': 3171}\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(class_counts.keys(), class_counts.values(), color='skyblue')\n",
        "plt.title(\"Distribución de clases en el dataset de entrenamiento\")\n",
        "plt.xlabel(\"Emoción\")\n",
        "plt.ylabel(\"Cantidad de imágenes\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(axis='y')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0394fa03",
      "metadata": {
        "id": "0394fa03"
      },
      "source": [
        "## Guardar el modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e70c81a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5e70c81a",
        "outputId": "4104af35-3ebc-41fa-c0e1-807ebd337cab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ],
      "source": [
        "model.save(\"emotion_model.h5\")\n",
        "# model.save(\"emotion_model.keras\") <-- Esta forma la recomienda keras ya que el que usamos se ve mas antiguo, pero ocupo confirmar con la documentación"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}